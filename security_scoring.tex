\section{Security metrics}\label{sec:androidvulnerability}
\label{sec:security_scoring}\label{sec:exp:security_score}

To allow buyers of Android devices to purchase those devices with the best security, they need to know how different device manufacturers, device models and network operators compare in terms of the security they provide.
We propose a method to score a device manufacturer, device model or network operator based on its historic performance at keeping devices up-to-date and fixing security vulnerabilities.
We find that Android as a whole gets a score of \daSecurityScore\ out of 10, the highest scoring device manufacturer is \emph{\daSecScoreBestmanufacturer} (\daSecScoreBestmanufacturerScore\ out of 10) and the lowest scoring is \emph{\daSecScoreWorstmanufacturer} (\daSecScoreWorstmanufacturerScore\ out of 10).

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figures/proportioninsecure}
\caption{Proportion of devices running insecure, maybe secure and secure versions of Android.
Table~\ref{tab:andvulns} lists the \daNumVulnsUsed\ vulnerabilities used, the red vertical lines are caused by their discovery and the most important are annotated.}
\label{fig:proportioninsecure}
\end{figure}
By combining data on critical vulnerabilities in Android and the versions of Android running on devices we can determine which vulnerabilities each device was vulnerable to each day.
Then we consider a device is insecure if it is running a vulnerable version of Android and the device has not received an update which might fix it;
it is maybe secure if it is running a vulnerable version but received an update which could have fixed the vulnerability if it contained a backported fix;
and it is secure if it is running a secure version.
This allows us to plot Figure~\ref{fig:proportioninsecure}, initially all devices are \emph{maybe secure} (yellow) since \da\ does not have historical data prior to May 2011.
This means we cannot distinguish between devices which are running a version of Android which is known to be vulnerable from one which may have received a backported fix.
This demonstrates the importance of a longitudinal study: this type of analysis requires years of data.
Once \avovuln{zergRush} was discovered in October 2011 then most devices are recorded as \emph{insecure} (red) as they were exposed to that vulnerability.
The remaining devices were already running a version of Android which fixed the \avovuln{zergRush} vulnerability and are therefore marked as \emph{secure} (green).
From October 2011 until the discovery of \avovuln[APK_duplicate_file]{APK duplicate file} in February 2013 the graph shows progressive improvement as devices are upgraded.
This means more and more devices are marked as \emph{secure} because they are now running a secure version of Android, or marked as \emph{maybe secure} because they received an OS update that did not update to a known-good version of Android but which may still have included a backport of a fix, as the update was made available after the vulnerability was disclosed.
From February 2013 onwards regular discovery of critical vulnerabilities ensures that most devices are exposed to known critical vulnerabilities.
This gives us an average exposure to critical vulnerabilities of Android devices of \daMeanInsecurityPerc.
This is $1 - f$, the proportion of devices free from critical vulnerabilities, however this is only one component of a more robust score, the FUM score.

\subsection{Method: The FUM score}\label{sec:security_scoring:method}

Computing how good a particular device manufacturer or device model is from a security standpoint is difficult because it depends on a number of factors which are hard to observe, particularly on a large scale.
Ideally, we would consider both the prevalence of potential problems that were not exploited and actual security failures.
%\footnote{A perfectly secure operating system would among other things detect and prevent all phishing attacks, provide perfect principle of least privilege isolation, not have any vulnerabilities, instantly fix all discovered vulnerabilities, not allow any user data to be used in a way which the user does not approve of and be really easy for an ordinary person to use.}
However, in the absence of such data we propose a scheme for assigning a device a score out of ten based on data that can be observed, is based on previous metrics, and that we expect correlates with the actual security of the devices.

The FUM score is computed from three components:
\begin{description}
  \item[free $f$] The proportion of running devices free from critical vulnerabilities over time.
  This is equivalent to Acer and Jackson's proposal to measure the security based on the proportion of users with at least one unpatched critical vulnerability~\cite{Acer2010} and similar to the Vulnerability Free Days (VFD) score~\cite{Wright2014}.
  Unlike VFD, this is the proportion of running devices which were free from critical vulnerabilities over time, rather than the number of days which the device manufacturer was free from outstanding critical vulnerabilities, as that does not take account of the update process.
  \item[update $u$] The proportion of devices that run the latest version of Android shipped to any device produced by that device manufacturer.
  This is a measure of internal updatedness, so a low score would mean many devices are being left behind.
  This assumes that newer versions are better with stronger security.
  Historically, steps have been taken to improve Android security in newer versions so this assumption should generally hold, but sometimes new updates introduce new vulnerabilities.
  \item[mean $m$] The mean number of outstanding vulnerabilities affecting devices not fixed on any device shipped by the device manufacturer. This is related to the Median Active Vulnerabilities (MAV) measure~\cite{Wright2014} but is the mean rather than the median, since this gives a continuous value.
  An example is given in Figure~\ref{fig:mcalculation}.
%TODO should we compute the median instead?
\end{description}

\begin{figure}
\centering
%\includegraphics[width=\columnwidth]{figures/mcalculation}
\include{figures/mcalculation}
\caption{As vulnerabilities are discovered and patched the sum of known but unpatched vulnerabilities each day varies. From this we can calculate $m = (0 \times 3 + 1 \times 5 + 2 \times 10 + 3 \times 2) / 20 = 1.55$ For comparison VFD = 0.15 and MAV = 2. Example based on the one given by Wright~\cite{Wright2014}.}
\label{fig:mcalculation}
\end{figure}

These three metrics $f$, $u$ and $m$, together measure the security of a platform with respect to known vulnerabilities and updates.
$f$ is a key measure of the direct risk to users as if there is any known but unfixed vulnerability then they are vulnerable.
However it does not capture the increased risk caused by there being multiple known vulnerabilities, which gives an attacker more opportunities and increases the likelihood of a piece of malware having a matching exploit.
This is captured by the $m$ score, which measures the size of the device manufacturers queue of outstanding vulnerabilities but does not take into account the update process or measure the actual end user security.
Neither of these metrics capture whether devices are being left behind and not being kept up-to-date with the most recent (and hopefully most secure) version, which is captured by $u$.

We want to provide a score out of 10 as this is easy for phone buyers to understand because many ratings are given as a score out of 10.
Since $f$ is the most important metric we weight it more highly.
Since $m$ is an unbounded positive real number, we map it into the range (0--1].
This gives us the FUM score:

\begin{equation}
\mathrm{FUM\ score} = 4 \cdot f + 3 \cdot u + 3 \cdot \frac{2}{1+e^m}
\end{equation}

We can compute the uncertainty for $f$, $u$ and $m$.
$f$ is computed by taking the total secure device days and dividing it by the total insecure and secure device days.
The total secure device days and total insecure device days are both counting experiments and so their measurement error is their square root~\cite{Taylor1997}.
Since the numbers involved are large, the uncertainty in $f$ is small.
$u$ is computed by taking the sum of the proportions of devices running the most recent version each day, both the count of devices running the maximum version and total count have square root uncertainties.
$m$ is computed by counting the number of vulnerabilities which affected that entity and which have not yet been fixed on any device we have observed from that entity every day and averaging over observed days.
However, it could be that the entity has released a fix to some devices but we have not yet observed a device with that fix.
So the uncertainty in our measurement is the probability of not having observed a fixed device if a fixed device existed.
We assume that if the fix has been released then at least \daMeanOutstandingProbability\ of devices have the fix.
This represents a trade-off between a proportion so small that the fix has not really been deployed and a reasonable estimate of the error.
This gives an uncertainty of \daMeanOutstandingUncertaintyEquation\ where $n$ is the number of devices contributing to that day's data for each vulnerability outstanding each day.
The Python \texttt{uncertainties} library was used to propagate uncertainties through calculations.
This does not capture systematic errors.
For example, we do not include manufacturer specific vulnerabilities, but we expect that performance in fixing manufacturer specific vulnerabilities is strongly correlated with performance fixing vulnerabilities affecting all of Android.

\todolater{There must be some way of putting an error bound on our results by considering the error on the whole DA set and the size of the subset used for each device manufacturer and device model}

\subsection{Results: Security scores}\label{sec:security_scoring:results}
The scores across the whole of Android are that \daMeanInsecurityPerc\ of devices are exposed to known root exploits.
There are on average \daMeanOutstandingVulnerabilities\ outstanding vulnerabilities not fixed on any device.
On average \daUpdatednessPerc\ of devices run the most recent version of Android.
This gives a security score of \daSecurityScore\ out of 10.

However there are a wide variety of scores depending on the source of the device.
There is anecdotal evidence that Google's Nexus devices are better at getting updates than other Android devices because Google makes the original updates and ships them to its devices\footnote{\url{http://www.howtogeek.com/139391/htg-explains-why-android-geeks-buy-nexus-devices/}}.
Table~\ref{tab:sec_summary} shows that this is the case with Nexus devices getting much better scores than non-Nexus devices and Figure~\ref{fig:security_score_summary} shows this holds true over time.

Different device manufacturers have very different scores; Table~\ref{tab:sec_manufacturer} shows the scores for the \daNumSigManufacturers\ device manufacturers with a significant presence in our data with \emph{\daSecScoreBestmanufacturer} (\daSecScoreBestmanufacturerScore\ out of 10) scoring highest and \emph{\daSecScoreWorstmanufacturer} (\daSecScoreWorstmanufacturerScore\ out of 10) scoring lowest.
Device manufacturers are considered significant if we have data from at least \daSigNumDevices\ devices and at least \daSigNumDeviceDays\ days of contributions.
Additionally, for $m$ and $u$ we ignore the days with less than \daSigNumDevicesDay\ devices contributing to that day's score.

\daTabSecScoressummary
\daTabSecScoresmanufacturer
\daTabSecScoresmodel
\daTabSecScoresoperator

Even within device manufacturers, different models can have very different update behaviours and hence security.
Table~\ref{tab:sec_model} shows the results for the \daNumSigModels\ device models which have a significant presence by the same metric with \emph{\daSecScoreBestmodel} (\daSecScoreBestmodelScore\ out of 10) scoring highest and \emph{\daSecScoreWorstmodel} (\daSecScoreWorstmodelScore\ out of 10) scoring lowest.
We can then test whether this seems fair by comparing the version data for the highest and lowest scoring models.
Figure~\ref{fig:full_version_comp}c shows the full version distribution for \emph{\daSecScoreWorstmodel}, which we only observe running one version.
Figure~\ref{fig:full_version_comp}b shows the full version distribution for \emph{HTC Desire HD A9191}, which used to be our worst model and for which we have more historical data; it shows how it received one update at the beginning of 2012, which was deployed fairly rapidly to most devices, but received no further updates.
Figure~\ref{fig:full_version_comp}a shows the same information for \emph{\daSecScoreBestmodel} which received \daSecScoreBestmodelNumFullVersions\ different versions, some of which were only deployed to small numbers of devices, but the distribution for all devices regularly and rapidly transitions from one version to another before ending up on `4.3 JWR66Y'.
Both \emph{Galaxy Nexus} and \emph{HTC Desire HD A9191} device models start off with the full version string of `2.3.3 GRI40' but the \emph{\daSecScoreBestmodel} receives many more updates over the same time period.
Other models from the same manufacturer with similar model names to \emph{HTC Desire HD A9191} do much better such as the \emph{Desire HD}.

If update delays are due to the delay in manufacturers providing the update rather than in operators supplying the update and users installing the update, we would expect the update behaviour of devices with the same device model to be similar and rapid.
We found that within \daModelHalfDeployment\ days of the first observation of a new version on a device, half of all devices of that model have the new version (or a higher version) installed, and within \daModelFullDeployment\ days \daFullDeployedAt\ of devices have the new version (or a higher version).
This compares with the average rates of deployment for Android OS versions of 350 days for half and 1\,100 days for \daFullDeployedAt.
\todolater{Compute these values automatically and with error.}
There is a variation between device models, with the update being distributed to most devices quickly and others having a much slower roll out, but since some do update quickly the bottleneck is unlikely to be with the user.
Perhaps some device models are preferred by users who are more likely to install updates than others, however we do observe updates being rolled out to device models quickly and user behaviour is not beyond the control of the device manufacturer.
They could install updates automatically or pester the user into installing them, and at least some of them do pester, silent automatic updates do boost uptake~\cite{Duebendorfer2009}.

\begin{figure*}
 \centering
 \includegraphics[width=0.9\textwidth]{figures/full_version_comp.pdf}
 \caption{Full version distributions for the highest and lowest scoring models}
 \label{fig:full_version_comp}
\end{figure*}

We also analysed the \daNumSigOperators\ network operators with a significant presence in our data.
Table~\ref{tab:sec_operator} shows the results \emph{\daSecScoreBestoperator} (\daSecScoreBestoperatorScore\ out of 10) scoring highest and \emph{\daSecScoreWorstoperator} (\daSecScoreWorstoperatorScore\ out of 10) scoring lowest.
However, the score of a network operator is affected by the manufacturers of the devices which are in use on its network.
This is in turn affected by both the device models a network operator offers to users and upon user's choice of device models.
Hence, having a worse score does not necessarily mean that a network operator is worse, it could be that its users all pick phones from a worse device manufacturer, for example, because they were cheaper.
A network operator could use data from this paper to exclude insecure devices from those offered to consumers.
An added value analysis of network operators, which takes into account the device mix used by users of that network operator, would make it possible to determine whether a network operator is making the situation better or worse by the way it ships updates to users.
However our sample size is too small to do that because while we have significant numbers of devices for each of the device models (Table~\ref{tab:sec_model}) and for each of the network operators (Table~\ref{tab:sec_operator}), we would need a significant number of each model in each network operator.
A large enough sample size would contain at least 100 unique devices each day for each device model for each network operator, hence for our \daNumSigModels significant models and \daNumSigOperators significant network operators, a daily average of at least 18\,000 unique devices would be needed, for a period of at least a year.
Since the distribution of devices is unlikely to be uniformly distributed across device models and network operators a more realistic estimate is 100\,000 unique devices each day.
This is not an unobtainable number but it is two orders of magnitude more than is available in \da.
\todo{Could abbreviate if necessary}
%We do not attempt to disambiguate users behaviour in whether they install updates from network operators using rolling upgrades.

\subsection{Scores over time}
The scoring metric as originally computed, is averaged over the whole history of the device manufacturer, device model or network operator, it gives equal weight to both periods years ago and to the last few months.
If instead we take an exponential moving average of the daily score for days with more than \daSigNumDevicesDay\, devices when there have been at least \daSigNumDays\ consecutive days of data with that many devices then we can plot how this score has changed over time.
Equation~\ref{eq:rolling_update} shows how the value for a particular day ($v_i$) is computed from the previous day's value and the input for the current day ($n$) with an $\alpha$ of $1/\daSigNumDays$.
\begin{equation}
v_i = v_{i-1} (1 - \alpha) + n \alpha
\label{eq:rolling_update}
\end{equation}
Figure~\ref{fig:security_scores} shows this for manufacturers, device models, network operators and for Nexus and non-Nexus devices.
These show how the scores for different entities are different and change over time, while there is correlated behaviour for different entities (for instance, when new vulnerabilities affecting all Android are discovered), these lines still have crossings due to the different behaviour of the various entities.
It also shows that we do not have sufficient data for all the entities all of the time, resulting in gaps in the data.
The clearest results are for Figure~\ref{fig:security_score_summary} with a large gap between the scores for Nexus and non-Nexus devices across the whole data set.
\begin{figure}
\centering
\begin{subfigure}{\columnwidth}
\includegraphics[width=\columnwidth]{figures/security_score_manufacturer}
\caption{Device manufacturers}
\label{fig:security_score_manufacturer}
\end{subfigure}
%
\begin{subfigure}{\columnwidth}
\includegraphics[width=\columnwidth]{figures/security_score_model}
\caption{Device models}
\label{fig:security_score_model}
\end{subfigure}
%
\begin{subfigure}{\columnwidth}
\includegraphics[width=\columnwidth]{figures/security_score_operator}
\caption{Network operators}
\label{fig:security_score_operator}
\end{subfigure}
%
\begin{subfigure}{\columnwidth}
\includegraphics[width=\columnwidth]{figures/security_score_summary}
\caption{Nexus and non-Nexus devices}
\label{fig:security_score_summary}
\end{subfigure}
\caption{Security scores for device manufacturers, device models, network operators and Nexus devices. 95\% confidence intervals indicated.}
\label{fig:security_scores}
\end{figure}

\subsection{Sensitivity of scoring metric}
\daTabSpearmanRanks
\daTabChangeInScores
To evaluate whether the ranking of different manufacturers is sensitive to the form of the scoring metric we computed the normalised Spearman's Rank correlation coefficient between the lists ordered using different forms of the scoring metric, this is shown in Table~\ref{tab:spearman_ranks}.
The `equal' metric weights $f$, $u$ and $m$ equally rather than favouring $f$ and makes little difference.
Similarly weighting $u$ or $m$ more highly rather than $f$ makes little difference.
While the $f$, $u$ and $m$ components do have some correlation with the overall FUM score, the rankings produced vary substantially.
Changing the scoring metric also impacts the scores given for each entity Table~\ref{tab:change_in_scores} shows the mean impact on the scores.
This shows that $m$ tends to drag down scores.
\todolater{Give some of the actual orderings as well for the most important ones - no space}
\todolater{Sensitivity of time based vs. global scores - no time}


\subsection{Gaming the score}\label{sec:gaming}
If the comparative data given here is used to influence purchasing decisions then entities in the Android ecosystem might try to game the score rather than genuinely improve security.
$f$ is hard to game without doing a good job at security but it doesn't get any worse if there is already one known vulnerability and another is found.
A high value of $u$ could be achieved by only ever shipping one version but that would give low values for $f$ and $m$ (and not be attractive to new customers).
A high value of $m$ could be achieved by focusing on only one device at a time and ensuring that it gets updates but ignoring all others, but that would lower $f$ and $u$.
One way to influence our scores would be to add additional devices to Device Analyzer, which have good security, these would have to be real end user devices since we could detect fake ones if they deviated from the behaviour of real devices in \da.
This would increase the size of our data set and would require providing genuinely good security to some users.
Some active attacks like blocking access to the \da\ servers from the mobile data network would not be effective as \da\ would retry on Wi-Fi.
Other denial of service attacks on the \da\ servers might be effective but illegal.
Some entities might be able to force the uninstallation of the app from all devices.
Therefore, our score is secure against passive gaming attacks which change the measured distribution, but would require active defence against active gaming attacks, which target the measurement devices.

\subsection{Utilitarianism}
From a utilitarian standpoint, while small manufacturers like Symphony and Walton do badly on our scores, they do not have as many customers as higher scoring manufacturers.
Hence the total risk to users from the higher scoring popular manufacturers is higher than the risk from the lower scoring unpopular manufacturers.
We could normalise the scores for market penetration and so give a score reflecting the risk posed by that manufacturer's performance, which would tend to decrease the difference between manufacturers in our current scoring.
Additionally since our scores are provided so that customers can chose which devices to buy then it is the marginal risk to that individual of that device which is of interest rather than the aggregate risk to all users.